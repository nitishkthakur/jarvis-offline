{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8666254d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_tavily import TavilySearch\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "# Initialize TavilySearch with the API key from environment variables\n",
    "os.environ[\"TAVILY_API_KEY\"] = os.getenv(\"TAVILY_API_KEY\")\n",
    "import httpx\n",
    "from typing import Optional, List, Dict, Any\n",
    "import arxiv\n",
    "from io import BytesIO\n",
    "import re\n",
    "import asyncio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c37a9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool = TavilySearch(\n",
    "    max_results=2,\n",
    "    topic=\"general\",\n",
    "    include_raw_content = True\n",
    "    # include_answer=False,\n",
    "    # include_raw_content=False,\n",
    "    # include_images=False,\n",
    "    # include_image_descriptions=False,\n",
    "    # search_depth=\"basic\",\n",
    "    # time_range=\"day\",\n",
    "    # include_domains=None,\n",
    "    # exclude_domains=None\n",
    ")\n",
    "\n",
    "tool = TavilySearch(\n",
    "    max_results=5,  # Increase for better chances of finding good content\n",
    "    topic=\"general\",\n",
    "    include_raw_content=False,  # Start with False - use summary first\n",
    "    include_answer=True,  # Let Tavily synthesize an answer\n",
    "    search_depth=\"advanced\",  # Better content extraction\n",
    "    include_domains=[\"arxiv.org\"],  # Restrict to arxiv for academic papers\n",
    "    # exclude_domains=[\"twitter.com\", \"reddit.com\"]  # Filter out low-quality sources\n",
    ")\n",
    "\n",
    "#res = tool.invoke({\"query\": \"Explain the first LLAMA paper in detail\"})\n",
    "\n",
    "res = tool.invoke({\"query\": \"LLaMA Large Language Model Meta arxiv\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befa8acd",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (378857508.py, line 12)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mif content_type ==\u001b[39m\n                       ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "final_links = []\n",
    "for r in resp['results']:\n",
    "    # Extract URL\n",
    "    url = r.get('url', 'No URL found')\n",
    "\n",
    "    # Extract the type of content\n",
    "    index_where_arxiv = url.find(\"arxiv.org/\") + len(\"arxiv.org/\")\n",
    "\n",
    "    # # Content type\n",
    "    last_half = url[index_where_arxiv:].split(\"/\")\n",
    "    content_type = last_half[0] if index_where_arxiv != -1 else \"Unknown\"\n",
    "\n",
    "    if content_type == \"pdf\":\n",
    "        final_links.append(url)\n",
    "\n",
    "    else:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d12f339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Using PyPDF2 (simple, good for basic text extraction)\n",
    "async def get_arxiv_text_pypdf2(arxiv_url: str) -> Dict[str, any]:\n",
    "    \"\"\"\n",
    "    Extract text from arXiv PDF using PyPDF2.\n",
    "    Install: pip install PyPDF2\n",
    "    \"\"\"\n",
    "    import PyPDF2\n",
    "    \n",
    "    try:\n",
    "        # Get PDF bytes\n",
    "        arxiv_id = extract_arxiv_id(arxiv_url)\n",
    "        pdf_url = f\"https://arxiv.org/pdf/{arxiv_id}.pdf\"\n",
    "        \n",
    "        async with httpx.AsyncClient() as client:\n",
    "            response = await client.get(pdf_url)\n",
    "            response.raise_for_status()\n",
    "            pdf_bytes = response.content\n",
    "        \n",
    "        # Extract text\n",
    "        pdf_file = BytesIO(pdf_bytes)\n",
    "        pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "        \n",
    "        text_content = \"\"\n",
    "        for page_num, page in enumerate(pdf_reader.pages):\n",
    "            text_content += f\"\\n--- Page {page_num + 1} ---\\n\"\n",
    "            text_content += page.extract_text()\n",
    "        \n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"text\": text_content,\n",
    "            \"num_pages\": len(pdf_reader.pages),\n",
    "            \"word_count\": len(text_content.split()),\n",
    "            \"method\": \"PyPDF2\"\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\"status\": \"error\", \"error\": str(e)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4a18ec01",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43masyncio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_arxiv_text_pypdf2\u001b[49m\u001b[43m(\u001b[49m\u001b[43marxiv_url\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhttps://arxiv.org/pdf/2302.13971\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/asyncio/runners.py:190\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(main, debug, loop_factory)\u001b[39m\n\u001b[32m    161\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Execute the coroutine and return the result.\u001b[39;00m\n\u001b[32m    162\u001b[39m \n\u001b[32m    163\u001b[39m \u001b[33;03mThis function runs the passed coroutine, taking care of\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    186\u001b[39m \u001b[33;03m    asyncio.run(main())\u001b[39;00m\n\u001b[32m    187\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    188\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m events._get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    189\u001b[39m     \u001b[38;5;66;03m# fail fast with short traceback\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    191\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    193\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Runner(debug=debug, loop_factory=loop_factory) \u001b[38;5;28;01mas\u001b[39;00m runner:\n\u001b[32m    194\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m runner.run(main)\n",
      "\u001b[31mRuntimeError\u001b[39m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": [
    "asyncio.run(get_arxiv_text_pypdf2(arxiv_url = \"https://arxiv.org/pdf/2302.13971\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b75bb523",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0x8f in position 10: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnicodeDecodeError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mhttpx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mhttps://arxiv.org/pdf/2302.13971\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mUnicodeDecodeError\u001b[39m: 'utf-8' codec can't decode byte 0x8f in position 10: invalid start byte"
     ]
    }
   ],
   "source": [
    "httpx.get('https://arxiv.org/pdf/2302.13971').content.decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149b561f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_arxiv_to_pdf(arxiv_url):\n",
    "    \"\"\"\n",
    "    Convert an arXiv URL to a direct PDF link.\n",
    "    \"\"\"\n",
    "    if \"arxiv.org/abs/\" in arxiv_url:\n",
    "        pdf_url = arxiv_url.replace(\"arxiv.org/abs/\", \"arxiv.org/pdf/\")\n",
    "        return pdf_url + \".pdf\"\n",
    "    return arxiv_url\n",
    "\n",
    "\n",
    "def scrape_arxiv_paper(link_to_arxiv_pdf):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61ead3f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'LLaMA Large Language Model Meta arxiv',\n",
       " 'follow_up_questions': None,\n",
       " 'answer': 'The LLaMA model is a large language model developed by Meta AI, released in 2023. It ranges from 7B to 65B parameters, and is known for its efficient training using publicly available datasets. LLaMA models have shown competitive performance compared to other large language models.',\n",
       " 'images': [],\n",
       " 'results': [{'title': 'arXiv:2402.07950v1 [cs.CR] 10 Feb 2024',\n",
       "   'url': 'https://arxiv.org/pdf/2402.07950v1',\n",
       "   'content': 'The model was named as LLaMA (Large Language Model Meta AI) [12, 13] and was released in February 2023, and the second model called LLaMA-2 [14], in ... An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models.\" arXiv preprint arXiv:2304.01933 (2023). [4] Chang, Yupeng, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie',\n",
       "   'score': 0.7482976,\n",
       "   'raw_content': None},\n",
       "  {'title': 'Large Language Models: A Survey - arXiv.org',\n",
       "   'url': 'https://arxiv.org/html/2402.06196v2',\n",
       "   'content': 'Large Language Models: A Survey Back to arXiv Back to arXiv This is experimental HTML to improve accessibility. Report Issue Back to Abstract Download PDF Table of Contents I Introduction II Large Language Models II-A Early Pre-trained Neural Language Models II-A1 Encoder-only PLMs II-A2 Decoder-only PLMs II-A3 Encoder-Decoder PLMs II-B Large Language Model Families II-B1 The GPT Family II-B2 The LLaMA Family II-B3 The PaLM Family II-C Other Representative LLMs III How LLMs Are Built III-A Dominant LLM Architectures III-A1 Transformer III-A2 Encoder-Only III-A3 Decoder-Only III-A4 Encoder-Decoder III-B Data Cleaning III-B1 Data Filtering III-B2 Deduplication III-C Tokenizations III-C1 BytePairEncoding III-C2 WordPieceEncoding III-C3 SentencePieceEncoding III-D Positional Encoding III-D1 Absolute Positional Embeddings III-D2 Relative Positional Embeddings III-D3 Rotary Position Embeddings III-D4 Relative Positional Bias III-E Model Pre-training III-F Fine-tuning and Instruction Tuning III-G Alignment III-H Decoding Strategies III-H1 Greedy Search III-H2 Beam Search III-H3 Top-k Sampling III-H4 Top-p Sampling III-I Cost-Effective Training/Inference/Adaptation/Compression III-I1 Optimized Training III-I2 Low-Rank Adaption (LoRA) III-I3 Knowledge Distillation III-I4 Quantization IV How LLMs Are Used and Augmented IV-A LLM limitations IV-B Using LLMs: Prompt Design and Engineering IV-B1 Chain of Thought (CoT) IV-B2 Tree of Thought (ToT) IV-B3 Self-Consistency IV-B4 Reflection IV-B5 Expert Prompting IV-B6 Chains IV-B7 Rails IV-B8 Automatic Prompt Engineering (APE) IV-C Augmenting LLMs through external knowledge - RAG RAG-aware prompting techniques IV-D Using External Tools Tool-aware prompting techniques IV-E LLM Agents Prompt engineering techniques for agents V Popular Datasets for LLMs V-A Datasets for Basic Tasks: language modeling/understanding/generation V-B Datasets for Emergent: ICL, reasoning (CoT), instruction following V-C Datasets for Augmented: using external knowledge/tools VI Prominent LLMs’ Performance on Benchmarks VI-A Popular Metrics for Evaluating LLMs VI-B LLMs’ Performance on Different Tasks VII Challenges and Future Directions VII-A Smaller and more efficient Language Models VII-B New Post-attention Architectural Paradigms VII-C Multi-modal Models VII-D Improved LLM Usage and Augmentation techniques VII-E Security and Ethical/Responsible AI VIII Conclusion -A LLM Training/Inference Frameworks -B Deployment Tools -C Prompting Libraries -D VectorDB License: CC BY 4.0 arXiv:2402.06196v2 [cs.CL] 20 Feb 2024 Large Language Models: A Survey Report issue for preceding element Shervin Minaee, Tomas Mikolov, Narjes Nikzad, Meysam Chenaghlu Richard Socher, Xavier Amatriain, Jianfeng Gao Report issue for preceding element Abstract Report issue for preceding elementLarge Language Models (LLMs) have drawn a lot of attention due to their strong performance on a wide range of natural language tasks, since the release of ChatGPT in November 2022. In this paper, we review some of the most prominent LLMs, including three popular LLM families (GPT, LLaMA, PaLM), and discuss their characteristics, contributions and limitations. We also give an overview of techniques developed to build, and augment LLMs.',\n",
       "   'score': 0.74652773,\n",
       "   'raw_content': None},\n",
       "  {'title': '[2402.06196] Large Language Models: A Survey - arXiv.org',\n",
       "   'url': 'https://arxiv.org/abs/2402.06196',\n",
       "   'content': \"View Jobs Skip to main content arXiv Is Hiring a DevOps Engineer View Jobs We gratefully acknowledge support from the Simons Foundation, member institutions, and all contributors.Donate >cs> arXiv:2402.06196 Help | Advanced Search Search GO quick links Login Help Pages About Computer Science > Computation and Language arXiv:2402.06196 (cs) [Submitted on 9 Feb 2024 (v1), last revised 23 Mar 2025 (this version, v3)] Title:Large Language Models: A Survey Authors:Shervin Minaee, Tomas Mikolov, Narjes Nikzad, Meysam Chenaghlu, Richard Socher, Xavier Amatriain, Jianfeng Gao View a PDF of the paper titled Large Language Models: A Survey, by Shervin Minaee and 6 other authors View PDFHTML (experimental) Abstract:Large Language Models (LLMs) have drawn a lot of attention due to their strong performance on a wide range of natural language tasks, since the release of ChatGPT in November 2022. LLMs' ability of general-purpose language understanding and generation is acquired by training billions of model's parameters on massive amounts of text data, as predicted by scaling laws \\\\cite{kaplan2020scaling,hoffmann2022training}. In this paper, we review some of the most prominent LLMs, including three popular LLM families (GPT, LLaMA, PaLM), and discuss their characteristics, contributions and limitations. We also give an overview of techniques developed to build, and augment LLMs. We then survey popular datasets prepared for LLM training, fine-tuning, and evaluation, review widely used LLM evaluation metrics, and compare the performance of several popular LLMs on a set of representative benchmarks.\",\n",
       "   'score': 0.72418857,\n",
       "   'raw_content': None},\n",
       "  {'title': 'arXiv:2302.13971v1 [cs.CL] 27 Feb 2023',\n",
       "   'url': 'https://arxiv.org/pdf/2302.13971v1',\n",
       "   'content': 'Meta AI Abstract We introduce LLaMA, a collection of founda-tion language models ranging from 7B to 65B ... critical when serving a language model at scale. In this context, given a target level of performance, ... to train a large model to reach a certain level of Equal contribution. Correspondence: {htouvron, thibautlav,gizacard,egrave',\n",
       "   'score': 0.7135319,\n",
       "   'raw_content': None},\n",
       "  {'title': 'arXiv:2302.13971v1 [cs.CL] 27 Feb 2023',\n",
       "   'url': 'https://arxiv.org/pdf/2302.13971',\n",
       "   'content': 'LLaMA: Open and Efficient Foundation Language Models Hugo Touvron ∗ , Thibaut Lavril ∗ , Gautier Izacard ∗ , Xavier Martinet Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozière, Naman Goyal Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin Edouard Grave ∗ , Guillaume Lample ∗ Meta AI Abstract We introduce LLaMA, a collection of founda-tion language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly avail-able datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community 1.',\n",
       "   'score': 0.69573146,\n",
       "   'raw_content': None}],\n",
       " 'response_time': 3.9}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7e55984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "![Ankur’s Newsletter](https://substackcdn.com/image/fetch/w_80,h_80,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2793f3d2-b75e-4405-abed-a419427aca14_900x900.png)\n",
      "\n",
      "# [Ankur’s Newsletter](/)\n",
      "\n",
      "#### Share this post\n",
      "\n",
      "![](https://substackcdn.com/image/fetch/w_520,h_272,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcc1c9214-566b-4a0d-8664-927b9fbf9fc2_1121x434.png)\n",
      "![Ankur’s Newsletter](https://substackcdn.com/image/fetch/w_36,h_36,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2793f3d2-b75e-4405-abed-a419427aca14_900x900.png)\n",
      "\n",
      "# LLaMA 1 vs LLaMA 2: A Deep Dive into Meta’s LLMs\n",
      "\n",
      "### Discover Meta’s journey into the world of LLMs and how LLaMA 2 compares to its successor, LLaMA 1.\n",
      "\n",
      "![Ankur A. Patel's avatar](https://substackcdn.com/image/fetch/w_36,h_36,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F96d4950c-656f-4af8-8188-48407e6bc8e5_612x612.png)\n",
      "![Saleem Maroof's avatar](https://substackcdn.com/image/fetch/w_36,h_36,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6ae3f092-ca81-42ab-b69f-e7394f841b9c_606x579.png)\n",
      "\n",
      "#### Share this post\n",
      "\n",
      "![](https://substackcdn.com/image/fetch/w_520,h_272,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcc1c9214-566b-4a0d-8664-927b9fbf9fc2_1121x434.png)\n",
      "![Ankur’s Newsletter](https://substackcdn.com/image/fetch/w_36,h_36,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2793f3d2-b75e-4405-abed-a419427aca14_900x900.png)\n",
      "\n",
      "## Key Takeaways\n",
      "\n",
      "Meta announced **LLaMA 1 in February 2023** as a response to OpenAI's and Google's language models. \n",
      "\n",
      "LLaMA 1 focused on efficiency by using fewer computing resourceswhile still maintaining high performance, comparable to GPT-3 despite being a smaller model. This model was only available through a **non-commercial license** for research purposes, primarily.\n",
      "\n",
      "Meta received over 100,000 requests to provide access to LLaMA 1, so they developed its successor: LLaMA 2. **LLaMA 2 was released in July 2023** and is available with a commercial license.\n",
      "\n",
      "LLaMA 2 was able to achieve **similar performance to GPT-3.5** on various academic benchmarks, but **performed much worse than GPT-4**, particularly in coding tasks.\n",
      "\n",
      "LLaMA 2 achieved much **lower safety violation scores** than its competitors, making it a big step forward in adhering to the ethical guidelines of LLMs.\n",
      "\n",
      "LLaMA 2 is **an improvement from LLaMA 1** in almost every aspect, and Meta will likely release a newer LLaMA model down the line.\n",
      "\n",
      "![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F5e7d2b37-027c-47ac-a30b-43f9e73f64df_2257x382.png)\n",
      "\n",
      "This post is sponsored by [Multimodal](http://multimodal.dev/), an NYC-based startup setting out to make organizations more productive, effective, and competitive using generative AI.\n",
      "\n",
      "Multimodal builds custom large language models for enterprises, enabling them to process documents instantly, automate manual workflows, and develop breakthrough products and services.\n",
      "\n",
      "[Visit their website](https://www.multimodal.dev/contact-us) for more information about transformative business AI.\n",
      "\n",
      "Ankur’s Newsletter is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.\n",
      "\n",
      "In July 2023, Meta announced LLaMA 2, an updated version of its predecessor, LLaMA 1. These large language models (LLMs) are Meta’s response to popular LLMs like GPT-4. LLaMA 2 is an open-source model available for commercial and research use.\n",
      "\n",
      "LLaMA 2 brings up the question of how it stacks up to LLaMA 1 and if it’s a significant improvement over the original model.\n",
      "\n",
      "Is it a massive leap forward from the original model, or just a small step in the ongoing evolution of LLMs?\n",
      "\n",
      "Let’s take a look at both of the LLaMA models to see if Meta’s latest LLM lives up to the hype.\n",
      "\n",
      "## LLaMA 1\n",
      "\n",
      "Before we take a look at LLaMA 2, we’ll need to have a look at its predecessor, [LLaMA 1](https://www.ankursnewsletter.com/p/llama-metas-open-source-rival-to), to make better comparisons. \n",
      "\n",
      "Meta first announced the release of LLaMA 1 in February 2023 in a [blog post](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/) and a research paper. It uses the transformer architecture and consists of four LLMs that have varying model sizes from 7B to 65B. \n",
      "\n",
      "It was **trained on 1.4 trillion tokens** from various publicly available online data sources, such as Common Crawl and Github. It also used Wikipedia in 20 different languages as a data source for training. One key advantage that LLaMA 1 has over its competitors is that it uses fewer computing resources. \n",
      "\n",
      "Notably, LLaMA 1 had high performance on various benchmarks in comparison to other LLMs like PaLM, Chinchilla, and GPT-3.\n",
      "\n",
      "This can be seen in the table below, which shows the performance of LLaMA 1 on common sense reasoning tasks compared to other LLMs.\n",
      "\n",
      "![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcc1c9214-566b-4a0d-8664-927b9fbf9fc2_1121x434.png)\n",
      "\n",
      "Additionally, LLaMA 1 has been fine-tuned to carry out various tasks. It also performed well on [other benchmark tasks](https://www.datacamp.com/blog/introduction-to-meta-ai-llama) such as reading comprehension, mathematical reasoning, and code generation. Even though LLaMA 1 was a smaller model than GPT-3, it still achieved similar performance.  \n",
      "\n",
      "Although it’s able to keep up with its LLM competitors when it comes to these benchmarks, it has the same issue of generating incorrect information.\n",
      "\n",
      "Specifically, LLaMA 1 was designed for research purposes, so it is only available through a non-commercial license, with researchers and developers having to apply for access.\n",
      "\n",
      "Now that we’ve explored the first version of LLaMA, let’s see how the second version builds upon this foundation.\n",
      "\n",
      "## LLaMA 2\n",
      "\n",
      "![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd2460d6b-093b-4013-b934-ff043e13eb9f_1559x847.png)\n",
      "\n",
      "With over 100,000 requests to access LLaMA 1, Meta decided to develop the open-source language model LLaMA 2 and release it in July 2023. Contrary to LLaMA 1, LLaMA 2 is available via a **commercial license** and through providers such as [Hugging Face](https://huggingface.co/meta-llama).\n",
      "\n",
      "As a result, researchers and developers are encouraged to collaborate to find new applications with LLaMA 2. Not only are the LLaMA 2 models open-source, but Meta also provides model weights and starting code for pre-trained LLaMA models. This lets developers and researchers build upon AI models more easily.\n",
      "\n",
      "Meta also developed a fine-tuned LLaMA 2 model called LLaMA-2-chat, which was trained on over 1 million human annotations. The primary use for this fine-tuned variant is in chatbot applications.\n",
      "\n",
      "### LLaMA 2 Training Process\n",
      "\n",
      "LLaMA 2 uses most of the same model architecture and presetting training as LLaMA 1.\n",
      "\n",
      "However, one major difference from LLaMA 1 is that LLaMA 2 used reinforcement learning from human feedback (RLHF) during its training process. As a result of learning through interactions with humans, the model is more helpful in conversations than LLaMA 1.\n",
      "\n",
      "LLaMA 2 was **fine-tuned with 40% more data than LLaMA 1**, where the data consisted of 1.4 million tokens. Moreover, the LLaMA 2 fine-tuned models are trained on 2 trillion tokens and have double the context length of LLaMA 1. These newer models have parameters ranging from 7B to 70B, while GPT-3 has 175B parameters.\n",
      "\n",
      "Similar to LLaMA 1, LLaMA 2 is efficient by providing high performance despite being a smaller model compared to its competitors.\n",
      "\n",
      "Additionally, Meta was cautious about using sensitive data during the training of LLaMA 2, so they removed data from sites that have a lot of personal information. This helps ensure LLaMA 2 follows ethical guidelines by avoiding the use of personal data.\n",
      "\n",
      "Safety was a huge priority for Meta during development, with LLaMA 2 achieving a much **lower violation percentage** when compared to other LLMs, with all the LLaMA 2 models staying below the 10% threshold. \n",
      "\n",
      "![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb82cd62f-6677-4c70-9ac9-75639b29d7b8_835x374.png)\n",
      "\n",
      "This is a huge step forward in LLM development, as the LLaMA 2-chat models drastically improve safety even when compared to closed-source models.\n",
      "\n",
      "### LLaMA 2 Performance\n",
      "\n",
      "When we take a closer look at LLaMA 2’s performance against GPT-4 in various academic benchmarks, LLaMA currently falls short.\n",
      "\n",
      "In particular, LLaMA 2’s performance on coding is lacking in comparison to other LLMs. LLaMA 2 can keep up with GPT-3 in almost every benchmark aside from coding, but has much lower performance than GPT-4 across the board.\n",
      "\n",
      "![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faeece447-296d-42fa-b4ed-a3885a543f76_1119x306.png)\n",
      "\n",
      "When it comes to comparing LLaMA 2 with closed-source powerhouse LLMs like GPT-4, it doesn’t perform as well yet. However, when compared to other open-source language models like Falcon, LLaMA 2 shows significantly better performance, making it a contender for some of the most powerful open-source models currently available.\n",
      "\n",
      "## Which one has the upper hand?\n",
      "\n",
      "Some of the major advantages of LLaMA 2 over its successor include:\n",
      "\n",
      "**Improved training and performance:** LLaMA 2 was trained on 40% more data and has twice the context length, so it can better understand complex language structures. It also performs better at [tasks](https://scontent-lhr8-2.xx.fbcdn.net/v/t39.2365-6/10000000_662098952474184_2584067087619170692_n.pdf?_nc_cat=105&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=ByL78P2ckIMAX_HcAzi&_nc_ht=scontent-lhr8-2.xx&oh=00_AfA37rooz3kg1gqYfqlTiK23fTjOgjqnsdkLftAIowyAyw&oe=64C84A3F) like reasoning, coding, and proficiency. \n",
      "\n",
      "**Open-source:** With LLaMA 2 being open-source and available for both commercial and non-commercial use, developers and researchers can find further innovative uses for the model. \n",
      "\n",
      "**Accessibility:** Meta partnered with various providers, such as Microsoft, to improve the accessibility of LLaMA 2. It’s available in the Azure AI model catalog and is optimized to run locally on Windows to provide a better experience for users. \n",
      "\n",
      "**Resources for responsible use:** With ethical concerns rising with recent AI usage, Meta provides various resources to ensure responsible use of their language model, including red-teaming exercises (testing the safety of the models by finding any weaknesses or vulnerabilities) and a transparency schematic.\n",
      "\n",
      "**Increased learning from human interactions:** The introduction of RLHF into the training process of LLaMA 2 makes it more proficient at learning from human interactions. \n",
      "\n",
      "Although LLaMA 2 has several advantages over LLaMA 1, the pretraining setting and model architecture are mostly the same as in LLaMA 1, with the main difference being the doubled context length.\n",
      "\n",
      "![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3fbeb4c0-881e-4b69-9c32-f4f189aecb97_1399x412.png)\n",
      "\n",
      "*The different parameters, context lengths, and tokens of the LLaMA 1 and LLaMA 2 models.* ([Source](https://scontent-lhr8-2.xx.fbcdn.net/v/t39.2365-6/10000000_662098952474184_2584067087619170692_n.pdf?_nc_cat=105&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=ByL78P2ckIMAX_HcAzi&_nc_ht=scontent-lhr8-2.xx&oh=00_AfA37rooz3kg1gqYfqlTiK23fTjOgjqnsdkLftAIowyAyw&oe=64C84A3F))\n",
      "\n",
      "Additionally, the bigger LLaMA 2 models (34B and 70B) use **grouped-query attention** (GQA). This allows for improved inference scalability, which is the model's ability to quickly process several prediction requests. \n",
      "\n",
      "While LLaMA 1 was mostly focused on academic purposes by being issued to research organizations upon request, LLaMA 2 takes the opposite approach by being available for research and commercial use.\n",
      "\n",
      "As a result, LLaMA 2 is a more versatile language model than LLaMA 1, due to its wider range of applications and use-cases. Some applications include content generation, personalized recommendations, and customer service automation.\n",
      "\n",
      "## Looking Ahead\n",
      "\n",
      "Meta’s answer to Google’s BARD and OpenAI’s ChatGPT has proved to be an interesting piece of the LLM puzzle. LLaMA 1 was already able to outperform GPT-3 with fewer computing resources.\n",
      "\n",
      "Although LLaMA 2 doesn’t outperform GPT-4, it still blows its open-source LLM competitors out of the water, especially when considering safety aspects. This makes LLaMA 2 a huge step forward in LLM development, as organizations now have the option to use a powerful open-source LLM that offers more customizability than popular closed-source models.\n",
      "\n",
      "With LLaMA 2’s increased training data, it’s able to perform significantly better than its predecessor in several benchmarks. Overall, LLaMA 2 is a more flexible and practical tool than LLaMA 1, with increased performance and accessibility through a commercial license.\n",
      "\n",
      "That raises the question: if Meta was willing to develop LLaMA 2 as a response to the 100,000 requests put in for access to LLaMA 1, does that mean we will see a LLaMA 3 at some point in the near future?\n",
      "\n",
      "It’s reasonable to assume Meta will continue its venture into the world of LLMs and eventually release a more refined LLaMA model.\n",
      "\n",
      "With LLaMA 2 currently unable to outperform its biggest competitor, GPT-4, there are high chances of Meta creating a new LLaMA model that can keep up with state-of-the-art closed-source LLMs.\n",
      "\n",
      "Ankur’s Newsletter is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.\n",
      "\n",
      "#### Share this post\n",
      "\n",
      "![](https://substackcdn.com/image/fetch/w_520,h_272,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcc1c9214-566b-4a0d-8664-927b9fbf9fc2_1121x434.png)\n",
      "![Ankur’s Newsletter](https://substackcdn.com/image/fetch/w_36,h_36,c_fill,f_auto,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2793f3d2-b75e-4405-abed-a419427aca14_900x900.png)\n",
      "\n",
      "#### Discussion about this post\n",
      "\n",
      "![User's avatar](https://substackcdn.com/image/fetch/w_32,h_32,c_fill,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack.com%2Fimg%2Favatars%2Fdefault-light.png)\n",
      "\n",
      "No posts\n",
      "\n",
      "Ready for more?\n",
      "\n",
      "#### Share\n"
     ]
    }
   ],
   "source": [
    "print(res['results'][0]['raw_content'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
